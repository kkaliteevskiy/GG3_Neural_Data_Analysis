{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6a89a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IIA project GG3: Neural Data Analysis\n",
    "\n",
    "Easter 2023<br>\n",
    "Project Leader: Yashar Ahmadian (ya311)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad546cd7",
   "metadata": {},
   "source": [
    "## Important dates\n",
    "\n",
    "Project start: __Thursday May 11 2023 9:30am GMT+1 (UK summer time)__ \n",
    "\n",
    "Interim report deadline: ðŸ”¥__Friday 19 May 2023, 4pm__ðŸ”¥ (electronic submission via Moodle)\n",
    "<br>\n",
    "(Interim report should contain report on tasks under the header \"**Week 1**\".\n",
    "\n",
    "Presentations: __Monday 5 June 2023, 11am-12:30pm in LT6__\n",
    "\n",
    "Final project report deadline: __Friday 9 June 2023,  4pm__ (electronic submission via Moodle)\n",
    "\n",
    "\n",
    "## Project notes\n",
    "\n",
    "- You should spend about 20 hours a week on the project, basically half of your time.\n",
    "- Project is to be carried out in **Google Colab** or on your own computer. You can download this notebook and use it with a normal Jupyter server, or duplicate it here in your **Colab** account. If you do the latter, you can share and show your work easily. The computational resources on **Colab** are limited, so you may find it more convenient to run the programs on your own computer, especially in the later parts of the project when computations will be heavier. When you need to ask a question about a specific piece of code, you can still use the **Colab** to share a notebook. \n",
    "- Weekly sessions will be held on Mondays 11:00-13:00, and Thursdays 9:00-11:00 and 14:00-16:00 all in **LR11** (EXCEPT for Monday 5th June which will be in LT6). \n",
    "- Attendance is compulsory for the first/introductory session and all Monday sessions. Thursday sessions will be optional (although this is still subject to change), but attending them is a good way to get answers to questions, some help with coding. It also provides space for teamwork with your teammates. \n",
    "- You are strongly encouraged to seek verbal feedback after your interim report - there will be a special session for this on Monday May 22 and Thursday May 25. \n",
    "- Project carries 80 marks overall:\n",
    "  - 20 marks for interim report (individual)\n",
    "  - 20 marks for presentation (group based)\n",
    "  - 40 marks for final report (individual)<br>\n",
    "  \n",
    "  \n",
    "### Project reports\n",
    "  - Should be clearly broken down by _Tasks_ (see below), any notes you wish to make in how you or your group structured and carried out the tasks, and most importantly your __results__ in the form of completely labelled graphs, and __accompanying conclusions__ you draw from your results. \n",
    "  - Interim report about 4-6 pages, and final report about 14-18 pages, when converted to a PDF (excluding appendices such as attached code, but _including_ figures). The final report can be an extension of the interim report, but make sure you take into account the feedback you receive for your interim report.  \n",
    "  - When deciding what to include in your report, how to organise it and what to emphasize, please prioritise communicating understanding over formalities - I would like give you marks for doing the right thing and showing that you did it and understand it. If I have to wade through pages of undigested data and graphs shown just because it was there, I will feel less generous. The length requirements are only guidelines. \n",
    "  - Take a look at [this page](http://teaching.eng.cam.ac.uk/node/444/#hdr-9) and [this](http://teaching.eng.cam.ac.uk/node/340) for further guidance and recommendations for writing reports.\n",
    "  - __All code__ that you used during to project must be attached as an appendix to your reports. If you modified one of the provided `.py` file (and you used that modified version for that report), include it. \n",
    "  - A jupyter or **Colab** notebook are acceptable as a report, as long as it is \"clean\" (its main section includes text and figures) and reads like a report, and (importantly) can be converted to a PDF, so you can upload it to the Moodle submission protal. \n",
    "  - Incude [cover sheets](http://teaching.eng.cam.ac.uk/node/4171) provided by the Teaching Office\n",
    "  \n",
    "### Presentations:\n",
    "- Each group will jointly prepare and present a 12 minute (strict!) presentation, broken up into three **4-minute parts** each delivered by one of the team members. There will be 3 minutes of question time after each talk, and so overall the session should take about an hour and a half. \n",
    "\n",
    "- Since most tasks are not really divided, the part presented by a student need not be something they solely contributed to. \n",
    "- The presentations will be held at the end of Week 3 or beginning of Week 4.\n",
    "- I will give guidelines and recommendations for making good presentations in due course. \n",
    "\n",
    "## Timeline\n",
    "\n",
    "See the Approximate Timeline section below.\n",
    "\n",
    "## Survey\n",
    "\n",
    "The **online survey** should be completed at the end of the project period. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f4b8e2-1bbf-4ef0-8e61-356e6533a848",
   "metadata": {},
   "source": [
    "# Neuroscience Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa168e21-72a4-43ca-9193-b06b23daf9ec",
   "metadata": {},
   "source": [
    "The background is also provided as a Jupyter notebook [accessible here](https://github.com/ahmadianlab/gg3_nda/blob/main/Background.ipynb). \n",
    "\n",
    "**Note:** The main point of the Background handout is to introduce some terminology (which appear all in boldface),<br>\n",
    "and mathematical notation that will be used in the next section, \"What is the right model of LIP?\"<br> \n",
    "Deep understanding of this Background  section is not required for carrying out the project. But, apart from the<br>\n",
    "above reason, you are encourged to read it to understand the scientific motivations and significance of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca6c12-5c30-4cc2-bbb1-29fb05547262",
   "metadata": {
    "tags": []
   },
   "source": [
    "# What is the right model of LIP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0a6cd-7d16-43f4-9549-cb99481b2d01",
   "metadata": {},
   "source": [
    "### Alternative hypothesis: stepping model\n",
    "\n",
    "As we saw in the Background section, classic studies suggested that LIP neurons which exhibit ramping activity in<br>\n",
    "their trial-averaged PSTH's are involved in evidence accumulation. However, the story became more complicated,<br>\n",
    "when in 2015, [Latimer et al.](https://www.science.org/doi/10.1126/science.aaa4056) provided evidence that most LIP neurons are better modelled<br>\n",
    " as neurons with a \"stepping firing rate\". In this alternative model\n",
    " the rate does not continuously ramp up or down<br>  (albeit via a random walk) as in a drif-diffusion model.\n",
    "Rather, the rate is piece-wise constant:<br> it starts relatively low, but at some time point it jumps (\"steps\") up discontinuously to <br>\n",
    "a higher firing rate level. The jump point is random and varies from trial to trial, according to some distribution. <br>\n",
    "\n",
    "#### -------------------------------   Figure 1   -------------------------------\n",
    "<img src=\"figs/latimer-step-ramp.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6abb3-eeb2-4630-8dfb-5ae866a8b81b",
   "metadata": {},
   "source": [
    "We will refer to these two competing hypotheses or models as the **ramping** and **stepping models**, respectively<br>\n",
    "(other common synonyms for the ramping model are \"the drif-diffusion model\", mentioned above, and \"the diffusion-to-bound model\";<br>\n",
    "we will also use **jump model** as synonymous with the stepping model.) \n",
    "\n",
    "In this project we aim to develop tools that allow us to reject or accept one of these hypotheses<br>\n",
    "based on observed spike trains. Understanding which of the two is a more accurate description of LIP activity <br>\n",
    "is scientifically significant. The ramping hypothesis suggests that LIP cortex is responsible<br>\n",
    "for accumulating evidence to inform and make decisions. On the other hand, the binary nature of the stepping model<br>\n",
    "suggests that LIP is downstream of the evidence accumulating area, and may simply reflect, in its activity, the decision already made<br>\n",
    "in an upstream area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d78609-a056-4ed8-a703-7baf368e61e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d3cc0-3549-4c0f-a568-cb20b12ef24b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "So far, our two \"models\" have mostly remained conceptual and qualitative. At this high, conceptual level <br>\n",
    "I will therefore refer to them as hypotheses instead: the ramping hypothesis vs. stepping hypothesis. <br>\n",
    "However, in order to use the powerful tools of probability theory and machine learning, we need to <br>\n",
    "turn these conceptual hypotheses into well-defined mathematical models.\n",
    "\n",
    "### The common, abstract model structure\n",
    "The ramping and stepping models to be described here and simulated in the project,  are examples of<br>\n",
    "**probabilistic generative models**. Each model has a set of **parameters** (such as the drift rate of the ramping model),<br>\n",
    "and it stochastically generates data, in our case spike trains. The systematic behaviour of these spike trains depends<br>\n",
    " on the various model parameters. Mathematically, this stochastic relationship between the parameters and data<br>\n",
    " is given by a conditional probability distribution\n",
    "\n",
    "$P(\\mathrm{data}| \\Theta, M)$\n",
    "\n",
    "where $\\Theta$ denotes the set of parameters and $M$ denotes the model (in our case $M$ = ramping, or $M =$ stepping).<br>\n",
    "This conditional probability, when viewed as a function of $\\Theta$, is called the model's **likelihood function**.<br>\n",
    "By (observed) \"data\" we mean a set of spike-trains recorded (in our case simulated) over many trials:\n",
    "\n",
    "$\n",
    "\\text{data} \\equiv \\{(n_t)_{t=1}^T\\}.\n",
    "$\n",
    "\n",
    "$n_t$ will sometimes be referred to as **observed variables**.\n",
    "\n",
    "**Latent variables:** As generative models, the two models can also be simulated to generate spike trains. In order to do this, <br>\n",
    "the two models first generate a firing rate function or time-series, $r_t$. The spike count, $n_t$, in a given<br>\n",
    "time bin is then stochstically generated based solely on $r_t$. The rate sequence $r_t$ is itself a stochastic<br>\n",
    "process, and depends on a set of *latent variables*. Latent variables are random variables that are so called<br>\n",
    "because they are not directly observed by us (data-)scientists and engineers, but need to be inferred from observed<br>\n",
    "data (the spike trains). In the simple version of the stepping model, with which we will start, there is only <br>\n",
    "a single latent variable: the stepping time. The ramping model, on the other hand, generates a whole sequence of <br>\n",
    "latent variables in each trial: these are the values of the ramping stochastic process, which is closely tied to the<br>\n",
    "firing rate.\n",
    "\n",
    "Since the latent variables (unlike the model parameters) vary from trial to trial, in each trial they need to be<br>\n",
    "inferred from a single spike-train. By contrast, parameters which control the systematic behaviour of the model will<br>\n",
    "be inferred from the entire dataset, i.e. the collection of spike trains in all trials.\n",
    "\n",
    "**Discrete vs continuous time:** Both models are implemented in discrete time. Thus the varible $t$ above is an integer<br>\n",
    "(index for the) time-step. We will denote the (fixed) total number of time steps in a trial by $T$. Real trials<br>\n",
    "last on the order of 1 second, and we would want our time steps or time bins to be around 1 to 10 milliseconds. <br>\n",
    "So, correspondingly, $T$ will be rather large, we will experiment with $T=$ 100 to 1000. For various purposes, <br>\n",
    "we will need to convert from discrete to continuous time in seconds. For that purpose we will fix the trial duration at<br>\n",
    "1 second and thus interpret each time-step to have duration $1/T$ seconds; we will denote this by $dt$ here and in the code<br>\n",
    "(thus $dt = 1/T$ seconds).\n",
    "\n",
    "We will now describe the probabilistic structure of the two models in some detail.\n",
    "\n",
    "### Stepping model\n",
    "\n",
    "**Latent variables:** This is the simpler one of the two. The only latent variable of this model is the step time or **jump time**. <br> \n",
    "I will denote the step time in trial $j$ by $\\tau_j$. Since we work in discrete time, $\\tau_j$ is a (non-negative) integer.<br>\n",
    "In `models.py` the corresponding variable is called `jump` or (when containing the value of multiple trials) `jumps`.<br>\n",
    "In each trial, the step timeÂ is sampled from some probability distribtion:\n",
    "\n",
    "$\\tau \\sim P(\\tau)$\n",
    "\n",
    "In the provided code this distribution is a so-called **negative binomial distribution** (see [this](https://en.wikipedia.org/wiki/Negative_binomial_distribution)) with two parameters: $m$ and $r$.<br>\n",
    "$m$ sets the average step time, and $r$ ... that's left for you to figure out.<br>\n",
    "\n",
    "As we said above, in each trial, the firing rate sequence of this model is piece-wise constant. If we denote the jump time of trial<br>\n",
    "$j$ by $\\tau_j$, then for $t < \\tau_j$, $r_t = R_0$ and for $t \\geq \\tau_j$, $r_t = R_h > R_0$, where the two constants $R_0$ and $R_h$ are <br>\n",
    "part of the model parameters. We will refer to them as pre- and post-step firing rates. \n",
    "\n",
    "Finally, given the rate sequence, $r_t$, the spike counts in different timesteps are generated indpendently from a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution):\n",
    "\n",
    "$n_t \\sim \\mathrm{Poiss}(r_t dt)$\n",
    "\n",
    "Note  that since we measure rates in Hz, $dt$ has to be in seconds, in order to get the right dimensionless parameter (mean spike count) of <br>\n",
    "of the Poisson distribution. \n",
    "\n",
    "**Fit parameters:** $m, r, \\text{ and } x_0$.\n",
    "\n",
    "These are the parameters which you aim to infer from spike train datasets.<br>\n",
    "$x_0$ is equivalent to $R_0$ and is given by $R_0/R_h$; think of it as the noramalised pre-step rate<br>\n",
    "(we use $x_0$ instead of $R_0$, to match the similar parameter in the ramping model). By definition $0< x_0 < 1$.\n",
    "\n",
    "**\"Fixed\" parameters:** We will take $R_h$ as known/fixed, and will not infer it from data. For many project tasks we will fix it at $R_h = 50$ Hz.<br>\n",
    "Though in early tasks you will explore its effects by varying it.\n",
    "\n",
    "### Ramping model\n",
    "\n",
    "**Latent variables:** This model, which approxmiates the continuous time drift-diffusion model, has a whole sequence of latent variables<br>\n",
    "which we will denote by $x_t$. This variable is the so-called **decision variable**. The update equations for $x_t$ are discretised versions<br>\n",
    "of the equation in Figure 3:\n",
    "\n",
    "$x_{t+1} = x_t + \\beta dt + \\sigma \\sqrt{dt} \\epsilon_t \\qquad\\qquad\\qquad$            Eq. (1)\n",
    "\n",
    "where \n",
    "\n",
    "$\\epsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0,1) \\qquad\\qquad\\qquad$            Eq. (2)\n",
    "\n",
    "(i.e. $\\epsilon_t$ is sampled independently in each time step from the standard normal distribution, $\\mathcal{N}(0,1)$, in other words, it has<br>\n",
    "a Gaussian distribution with mean 0 and variance 1.) The initial condition is set via\n",
    "\n",
    "$\n",
    "x_1 = x_0 + \\sigma \\sqrt{dt} \\epsilon_0\n",
    "$\n",
    "\n",
    "where $\\epsilon_0$ is again standard normal, and $x_0$ is a model parameter (and not a latent variable, <br>\n",
    "as it is the same across all trials). (Note that, due to python indices starting from 0, the equation above will (implicitly)<br>\n",
    "appear as `x[0] = x0 + sigma * np.random.randn()` in the code).\n",
    "\n",
    "The firing rate in this model is a rectified and scaled version of $x_t$:\n",
    "\n",
    "$r_t = R_h [x_t]_+ = R_h \\max(0, x_t)$\n",
    "\n",
    "It is not hard to see that the sequential variables $x_t$ form a Markov chain (this has to do with the fact that $\\epsilon_t$<br> in different\n",
    "trials are independent), and therefore the ramping model is an example of a **hidden Markov model (HMM)**.<br>\n",
    "In fact, if you have taken 3F8, you will realise that $x_t$ is *almost* an AR(1) Gaussian Process. I said almost an AR(1) Gaussian process, because<br>\n",
    "\n",
    "*when $x_t$ reaches 1, it will get stuck there for the rest of the trial*. (Equivalently, after this point, the firing rate, $r_t$, stays at its<br>\n",
    "maximal level $R_h$.)\n",
    "\n",
    "This reflects the interpretation of $x_t$ as a decision variable, which upon reaching a pre-set bound or threshold, triggers<br>\n",
    "the decision; in our case the bound is 1. \n",
    "\n",
    "Similar to the stepping model, given the rate sequence, $r_t$, the spike counts in different timesteps are generated indpendently from a Poisson distribution\n",
    "\n",
    "$n_t \\sim \\mathrm{Poiss}(r_t dt)$.\n",
    "\n",
    "**Fit parameters:** $\\beta, \\sigma, \\text{ and } x_0$.\n",
    "\n",
    "$\\beta$ and $\\sigma$ control the systematic drift vs. stochasticity of the ramping variable $x_t$.<br>\n",
    "Similar to the stepping model, $x_0$ sets the initial rate, $r_0$, via $r_0 = R_h x_0$. And again $0< x_0 < 1$.\n",
    "\n",
    "**\"Fixed\" parameters:**  $R_h$, maximal rate, to be treated as in the stepping time.\n",
    "\n",
    "\n",
    "### Ignored stimulus dependence\n",
    "\n",
    "In the full version of the ramping model, the magnitude and sign of $\\beta$ depends on the coherence and the direction of motion of the RDM stimulus<br>\n",
    "in that trial. However, for simplicity, in this project we assume $\\beta$ is fixed in all trials and assume it is positive.<br>\n",
    "\n",
    "Similarly, in the full version of the stepping model, the post-jump rate can take two possible values $R_h > R_0$, as described above, <br>\n",
    "or $R_l < R_0$. We can call $R_h$ and $R_l$ the up or down rates, and call their normalized values of 1 and $R_l/R_h < 1$ up and down states.<br>\n",
    "The probability with with the model transition up or down after the jump time can again depend on the coherence and direction of motion of the <br>\n",
    "RDM stimulus in a trial. But again, to simplify the model, we ignore this fact. In fact, for most of the project we work with a stepping model<br>\n",
    "without a down state. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9c313-952c-4beb-8c39-0100c23d5768",
   "metadata": {},
   "source": [
    "# Approximate timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be6ffc-eaf1-4d02-97cd-cd8a8d8d1f79",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our task in this project is to slowly build up techniques to ultimately reject or accept one or the other hypothesis based on \n",
    "recorded (or, in our case, simulated) spike trains. We will do this by progressively moving from lower to higher levels of \n",
    "probabilistic inference. \n",
    "\n",
    "\n",
    "- (week 1) explore the behaviour of the two models based on simulator code provided to you in `models.py`.<br>\n",
    "And take preliminary steps towards developing a discrete-state HMM approximation to them, which allows us to apply  powerful inference tools. \n",
    "\n",
    "- (week 2) develop tools to carry out **single-trial inference**  of the models' latent variables from observed  \n",
    "on single spike trains, taking advantage of their HMM formulation.\n",
    "\n",
    "- (weeks 2-3) Assuming model $M$ is the true model underlying data, use Bayesian or maximum-likelihood inference to infer or estimate<br>\n",
    "model parameters, $\\Theta$, based on observed data, that is, many trials of simulated spike trains. \n",
    "\n",
    "- (weeks 3-4) Use Bayesian inference to select/reject one or the other hypothesis/model, given a dataset of spike trains.<br> \n",
    "\n",
    "Depending on feedback and pace of progress, in week 4 we will also investigate the consequences of model mismatch. Since \"all models are wrong (but some are useful)\",<br> \n",
    "what can we say about the possibility of reaching wrong conclusions regarding our alternative conceptual hypotheses, due to <br>\n",
    "some arbitrary choices we had to make in translating those conceptual models to concrete mathematical models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33dd59-3fc8-4a7f-b6bf-7fdf540300d5",
   "metadata": {},
   "source": [
    "# Running on Colab vs Deepnote\n",
    "As I said in the Intro lecture, the project Jupyter notebook is accessible, via these links, on both [Deepnote] and \n",
    "on [Google Colab]. You can choose to run your notebook on either of those (for Deepnote you will need to sign up\n",
    "and create an account with them), or you can download it and run things on your own machine, which may be faster. \n",
    "\n",
    "If you choose to work on Deepnote you will need to \"Duplicate\" the notebook (using the blue\n",
    "button on the top right) so that you can make and save your changes. Similarly, if you use Colab, you will have to\n",
    "\"Save a copy in Drive\" in order to be able to save your changes (if you don't have Google Drive, then either sign up, or download and\n",
    "work on your laptop, or use Deepnote).\n",
    "\n",
    "Finally, the `.py` modules (including Week 1's `models.py`) are accessible and can be download from the Deepnote. \n",
    "On Colab, you will have to run the following cell to import them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa7c4a2-98f7-4f6d-8841-b74dc713750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"colab\" # change this to \"local\" if you are on Deepnote or your own computer\n",
    "\n",
    "if mode == \"local\":\n",
    "    import models\n",
    "elif mode == \"colab\":\n",
    "    import requests\n",
    "    url = 'https://github.com/ahmadianlab/gg3_nda/blob/main/models.py?raw=true'\n",
    "    r = requests.get(url)\n",
    "    with open('models.py', 'w') as f:\n",
    "        f.write(r.text)\n",
    "    import models\n",
    "else:\n",
    "    raise Exception(\"mode must be either local or colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d9133-5bd6-4f34-a680-0268aecef4d6",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "### model simulation and behaviour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df778a9b-3b9c-4330-b5fa-53a9945c2051",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "Study the code in `models.py`, specifically the implementations of the two models in the `StepModel` and `RampModel` classes. <br>\n",
    "The main part to study (and relate to the mathematical discussion above) is their `simulate` method/function. You create <br>\n",
    "an object instance of each model by providing the model parameters (both \"fit\" and \"fixed\" parameters, as named above)<br>\n",
    "to the class constructors: e.g. `ramp = RampModel(beta=...)`. <br>\n",
    "(Ignore the other input arguments in the class constructor `__init__` for now, and leave them at their default values.)<br>\n",
    "Once a model object is created you can use its `simulate` method to get an array of spike trains over multiple trials. <br>\n",
    "(For usage see the docstring (or run help via `ramp.simulate?`.) `simulate` will also return the generated latent variables, <br>\n",
    "and, optionally, the firing rates in different trials. \n",
    "\n",
    "Visualise the simulated spike trains by writing code to make so-called \"spike raster\" plots. See the bottom row of Figure 5<br>\n",
    "above for example spike raster: different rows represent the spike trains in different trials, and spikes are shown by dots. <br>\n",
    "(you can put a dot for every nonzero $n_t$, even if the nonzero value is more than 1; this is unlikely if you keep `Rh` below<br>\n",
    "50 Hz and use a `T` of at least 100 (recommended). At this stage it should not be time-consuming to use higher `T`'s as well,<br>\n",
    "e.g. `T = 1000` (corresponding to 1 millisecond time-steps). If you are simulating hundreds of trials, you don't want to include<br>\n",
    "all of them in the raster. Use your common sense to decide how many trials to include in the raster; this a visualisation tool used to get<br>\n",
    "an idea of how spike trains behave qualitatively by seeing a good number of example. \n",
    "\n",
    "Vary the parameters of each model and generate spike rasters in different regions of the parameter space, trying to find<br>\n",
    "qualitatively different behavior. The default values of the parameters give you a first guess or the right order of magnitude for the <br>\n",
    "different parameters. (For `m` and `r` of the step model, note that they should scale with the `T` you will be using for the simulation;<br>\n",
    "in particular, for more interesting/relevant results, you would want to set `m` at or near `T / 2` so that the steps happen on average in the middle of the trial.)\n",
    "\n",
    "What systematic patterns can you detect? \n",
    "\n",
    "Write code to also mark the jump times in different trials over the spike trains in the raster. <br>\n",
    "Also make histograms of jump times. What is the effect of the `r` parameter on the behaiour of the stepping model?<br>\n",
    "\n",
    "Similarly make plots of the trajectories of $x_t$ or $r_t$ (of the ramp model) in several trials, in a single plot.<br>\n",
    "You can extract the time when $x_t$ of the ramping model hits its upper bound of 1 (equivalently $r_t$ reaches $R_h$), and histogram that as well.<br> \n",
    "How do `beta` and `sigma` affect this histogram or the behaviour of the $x_t$ trajectories?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ba518-9a6b-4eac-87f1-b3d2c7477733",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "PSTH is an important data analysis tool used in neuroscience.<br> \n",
    "This  is a statistical estimate of the trial-averaged firing rate as a function of time, based on recordings of spike trains in<br>\n",
    "multiple experimental trials. It is obtained by binning/histogramming spikes (e.g. using `np.histogram`) in different time bins<br>\n",
    "and averaging the resulting spike counts over many trials (you can also divide by `dt` to turn into rate in units of Hz).<br>\n",
    "\n",
    "Write code to construct and plot PSTH's in different regions of each model's parameter space. Note how the PSTH <br>\n",
    "fluctuates randomly from dataset to dataset. It is better to do some sort of (temporal) smoothing in order to reduce these<br>\n",
    "fluctuations and the jaggedness of the PSTH. You can use either a sliding window (e.g. a boxcar window/functin) averaging, or simply <br>\n",
    "use time bins that are larger than the oridinal time steps (e.g. 50 milliseconds -- or 5 timesteps if you are using a `dt` of 10 ms,<br> \n",
    "corresponding to `T = 100`). The smooth ramping firing rate curves in Figure 4 of the [Background](https://github.com/ahmadianlab/gg3_nda/blob/main/Background.ipynb)\n",
    "are examples of smoothed PSTH's.\n",
    "\n",
    "Even with the smoothing there will be fluctuations in the PSTH from dataset to dataset. How does the strength of these fluctuations depend on (or scale with)<br>\n",
    "the number of trials (in each dataset)? Try to be quantitative about this, e.g. by using informed plots. <br>\n",
    "For the rest of this task use a high number of trials (e.g. 5000) to minimise these fluctuations. (But note <br>\n",
    "that in real experiments the number of trials rarely exceeds a few hundred -- for for later tasks we will bring the number down.)\n",
    "\n",
    "\n",
    "Finally, try to find parameter regimes that make the PSTH of the stepping model very close to that of the ramp model. (First make sure<br>\n",
    "the ramp model's PSTH look qualitatively like the classic ramping PSTH's in LIP experiments.) In which parameter regions<br>\n",
    "does this fail drastically, and in which regimes are the two PSTH's nearly indistinguishable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb66917-0d16-4cb3-8946-63243c4918aa",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "The PSTH is an example of a so-called first-order statistic, in that it is the averge of spike counts, $n_t$, which is their first moment. <br>\n",
    "You can also evaluate higher order statistics, such as the variance of $n_t$ (across trials).<br>\n",
    "Instead of smoothing, for evaluating the variance use larger time bins (e.g. 50 or 100 milliseconds).\n",
    "\n",
    "How does the variance behave as a function of time and of various parameters in each model? \n",
    "\n",
    "A more useful quantity is the Fano factor which is the ratio of the variance of $n_t$ to its mean (obviously both evaluated in the same time bin, in particular<br>\n",
    "time bins of the same width). This quantity is 1 for the Poisson distribution (the default choice for the emission distribution of both models).<br>\n",
    "Evalute and plot the Fano Factor as a function of time, and again investigate how it changes in different parameter regimes, and importantly<br>\n",
    "whether and how it behaves differently in the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1821f-ef7e-436e-a443-18e45c38883f",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "\n",
    "(This is a more open-opended and less guided task compared to the previous ones. Use brainstorming in the group and come up with creative ideas to address this, working together.)\n",
    "\n",
    "In this task you will explore an informal or relatively ad-hoc version of what we intend to do eventually using the systematic approach of Bayesian inference. <br>\n",
    "The aim is to find an intelligent but ad-hoc (in the sense that it is not Bayesian and does not rely on the two models' likelihood function, but only relies on \n",
    "observed statistics) way of telling the two models apart, i.e. deciding which model generated a dataset.\n",
    "\n",
    "Relying on 1st and 2nd order statistics you have explored, or other 2nd order statistics and perhaps higher order statistics you can come up it,\n",
    "construct a criterion (or multiple alternative criteria which you would then compare) for deciding between the two models. To make this challening, \n",
    "you will obviously need to put the two sets of model parameter in respective regimes in which the two models are least distinguishable based their generated spike trains. \n",
    "Also try to come up with criteria that are relatively robust, i.e. are not handcrafted to tell the two models apart only for a fixed choice of parameters.\n",
    "\n",
    "You have to test your criterion by running it on several datasets, once generated by the ramp model, and in another round, generted by the step model. \n",
    "And then quantify what percent of datasets in each case where classified/decided correctly. Use a number of trials not more than 400 for each dataset you will run your test/criterion on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af18d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Week 2\n",
    "### Latent variable inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7794027e-ee22-44ab-9da2-d784dddedffc",
   "metadata": {},
   "source": [
    "In Week 2, your task is to formulate the two models as (time-homogeneous) **Hidden Markov Models (HMM)** with *discrete* states. To recap, <br>\n",
    "a HMM is described by latent variables that are temporal/sequential and form a **Markov chain (MC)**.<br> \n",
    "If we denote these variable at time step $t$ by $x_t$, the Markovian property means that conditioned on $x_t$, all future states<br>\n",
    "$(x_{t+1}, x_{t+2}, \\ldots)$ are independent of states at times before $t$.<br>\n",
    "Colloquially, conditioned on the present, the future is independent of the past.<br> \n",
    "\n",
    "In an HMM, the Markovian state variables are not directly observerd. Instead at each time, we observe an observed variable, $n_t$, which<br>\n",
    "only depends of the Markov state, $x_t$, at the same time step. For us this dependence is given by the Poisson distribution describing the <br>\n",
    "spike emissions, conditioned on the rates, with the latter being determined by $x_t$ (in other words, the rate at time $t$ is a<br>\n",
    "deterministic instantaneous functions of $x_t$).<br> Figure 2 shows the graphical model for the HMM, with such Poisson observation (aka emission) distributions.  \n",
    "\n",
    "Here, we aim to design Markov chains with discrete states to approximate the behavior of the latent variables of the step and ramp models.<br>\n",
    "In the case of the step model, the discretization is actually exact, as the model really just has two levels of rates (although we will see that the correspondence between Markov states and possible firing rate levels is rather complicated). <br>\n",
    "It is for the ramp model (which in its original formulation has continuous states, $x_t$) that a discretization will be an approximation. \n",
    "\n",
    "The reason for formulating the models as discrete state HMMs is that in this case we can use the powerful and efficient <br>\n",
    "forward-backward algorithm to calculate<br> (1) the Bayesian *posterior* estimate (the posterior mean) of the state variables, $x_t$, given the observations, $n_t$, and<br>\n",
    "(2) calculate the model likelihood function,  $P(n_{1:t}|\\Theta, M)$, i.e. the probability of the observed spike train conditioned on the model parameters.\n",
    "\n",
    "\n",
    "<img src=\"figs/HMM_graph.png\" width=600/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168eedf8-5a3d-41e6-87f4-e06d1e91523f",
   "metadata": {},
   "source": [
    "### Task 2.1: \n",
    "**Forming an HMM approximation to the ramp model**\n",
    "\n",
    "$\\newcommand{\\T}{\\mathcal{T}}$\n",
    "\n",
    "The state variable of the (original) ramp model is continuous and because the update rule $x_{t+1} = x_t + \\beta dt + \\sigma\\sqrt{dt}\\epsilon_t$<br>\n",
    "involves the Gaussian variables $\\epsilon_t$, the transition probability $P(x_{t+1}| x_{t})$ is Gaussian (with mean and variance possibly depending on $x_t$, as well as the model parameters). Your first sub-task is to work out this distribution.\n",
    "\n",
    "First, we will approximate $x_t$ (which lies between 0 and 1) to take values on a regular grid of $K$ points going from 0 to 1 (inclusive of both ends). Let's use $K=50$ or $K=100$.<br>\n",
    "These $K$ points form the disrete states of our approximate discrete-state HMM. We will denote these states by their index $s$ going from 0 to $K-1$.\n",
    "The corresponding value of $x_t$ is then given by \n",
    "\n",
    "$x_t = \\frac{s_t}{K-1} \\qquad \\text{where} \\quad s_t \\in \\{0, \\ldots, K - 1\\}$.\n",
    "\n",
    "By evaluating the Gaussian distribution you derived on this grid, form the transition matrix for the Markov chain, defined as \n",
    "\n",
    "$\n",
    "\\T_{s,s'} = P(s_{t+1}= s'| s_{t}= s)\n",
    "$\n",
    "\n",
    "Note that the state *transitioned to* (i.e. the one at $t+1$) is the column index of the matrix. Thus the rows of $\\T$ have to sum up to 1 (as this is a sum over columns).<br> \n",
    "You need to **enforce this constraint by hand** after constructing the (intial) matrix using the evaluated Gaussian. <br>\n",
    "\n",
    "Also note that according to the original model, once the variable $x$ reaches 1, it stays there. So you will need to use a separate way<br> to construct the last row of $\\T$, corresponding to transitions out of state $s = K-1$, corresponding to $x=1$.\n",
    "\n",
    "Next, you will need to form the initial state distribution, $\\pi$, as an array of $K$ values (summing to 1!) giving the probabilities of different possibilities of $s_0$. This should approximate the equation $x[0] = x_0 + \\sigma\\sqrt{dt} \\epsilon_0$.\n",
    "\n",
    "Once the transition matrix $\\T$ and initial state distribution $\\pi$ are formed, we can simulate the chain. In order to <br> do this, you will first draw $s_0$ from the initial state distribution, then successively sample from the <br>\n",
    "appropriate distribution according to the transition matrix $\\T$ (and depending on the current state $s_t$). To sample <br>\n",
    "the discrete (integer) $s$ from a distribution over its $K$ possibilities, you can use `np.random.choice`.\n",
    "\n",
    "For different choices of $\\beta$, $\\sigma$ and $x_0$, simulate several trials of this chain and (after rescaling) plot the trajectories $x_t$. Compare with corresponding simulated trajectories of the original (continuous state) model, to make sure your implementation is accurate enough. \n",
    "\n",
    "Finally, given each $x_t$ you can calculate the corresponding rate $r_t$ and (after multiplying by $dt$!) obtain the mean count in each time bin/step and draw a sample from the corresponding Poisson distribution to obtain a sample of $n_t$.<br>\n",
    " For this, use the function `inference.poisson_logpdf` from the provided new module `inference.py`.\n",
    "Simulate various spike trains, plot their rasters, PSTH's and Fano Factor plots, and compare with the corresponding results from the original model.\n",
    "\n",
    "\n",
    "**Note:** if you use values of $\\sigma$ that are too small, and depending on your code for constructing $\\T$, your code for generating $\\T$ or $\\pi$ may run into numerical truncation issues resulting in `NaN` values. Something that could help is implementing things first in terms of log-probabilities, using the numerically stable function `scipy.special.logsumexp` in the normalization step (when you normalize rows of $\\T$ or the vector $\\pi$), and only in the end exponentiating to obtain the actual $\\T$ or $\\pi$. Another solution (recommended) is to not use very small values for $\\sigma$ and/or `dt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9552450-cd09-451f-9951-0882b1dd4f56",
   "metadata": {},
   "source": [
    "### Task 2.2: \n",
    "**Forming an HMM approximation to the step model**\n",
    "\n",
    "Intuitively, given its discrete (binary) rate levels, the step model can actually be exactly formulated as a discrete state HMM.<br>\n",
    "\n",
    "First, implement a *time-homogeneous* Markov chain representing the step state with two states. How would you choose the transition probabilities?<br> (Hint: think of the \n",
    "parameter $p$ of the Negative Binomial distribution, which in terms of $m$ and $r$ is given by $m/(m+r)$).\n",
    "\n",
    "Simulate the Markov chain for several trials and plot the corresponding $x_t$ trajectories. \n",
    "\n",
    "Also evaluate the jump times (time-steps) and make histograms of these jump times. How do the histograms appear? Do they resemble any of the histograms of jump times<br> (meaning histograms \n",
    "corresponding to different values of $m$ and $r$)  you made in Week 1? \n",
    "\n",
    "What do you think is wrong with the 2-state Markov chain approximation to the step model? **Â§Â§**\n",
    "\n",
    "\n",
    "\n",
    "Read about the [Negative Binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution) and its \"meaning\" (*for the case of positive integer $r$*), to get clues for constucting an exact Markov Chain formulation of the step model (Hint: use $r + 1$ states!).\n",
    "\n",
    "Again, simulate several trials of this new chain, plot state trajectories, and form histograms of jump times for different values of $r$. How do these compare with the histograms you obtained in Week 1 for jump times of the original step model?\n",
    "\n",
    "Finally, as you did for the ramp model, construct the full HMM model corresponding to this modified Markov chain, and as in Task 2.21, make rasters, PSTH, and FF plots for the obtained spike trains, and compare with the original model. \n",
    "\n",
    "**Â§Â§ Note:** as found by one of the groups on Monday, an exact fomrulation of the step model is possible as a *time-inhomogenous* 2-state Markov chain (MC). You can experiment with that of course (this is optional). However, we will be using a time-homogenous HMM, and so in the above sub-task I am asking you to construct a time-homogenous Markov chain. If you did implement a time-inhomogeneous MC formulation, feel free to write about it in your final report. But make sure you do investigate the time-homogeneous version (which is in general not a correct formulation of the original model) as well, and answer the questions for that (too)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f821da5-d58b-4fb0-a830-9e9e519db23e",
   "metadata": {},
   "source": [
    "### Task 2.3: \n",
    "**Inference of hidden states**\n",
    "\n",
    "Henceforth, for the rest of the activities this week (as well as most of the activities of the next two weeks), we will be solely working with the discrete-state HMM versions of the two models that you have implemented (instead of using the `models.py` simulators).\n",
    "\n",
    "The `hmm_expected_states` function (see its doc/help) in the `inference.py` (run the next code cell to import this on Colab **Â§**) module implements the forward-backward algorithm (FBT) to calculate the posterior probabilities $P(s_t | n_{1:T})$, as well as the log-likelihood $\\ln P(n_{1:T})$.**Â§Â§**<br>\n",
    "Use this function to obtain these posterior probabilities for your finite-state MC implementations of both models. \n",
    "\n",
    "- Write code to calcualte the posterior expectation of $x_t$ (i.e. $\\mathbb{E}[x_t | n_{1:T}]$), for the ramp model, based on the posterior probabilities $P(s_t | n_{1:T})$. As in task 2.1, generate several trial spike trains using the discrete-state HMM ramp model (with the corresponding $x_t$ trajectories retained), and for each trial infer and plot $\\mathbb{E}[x_t | n_{1:T}]$, together with the ground-truth simulated $x_t$. Repeat this in different regions of model parameter space (including low and high values of $x_0$ and $R_h$). In what parameter regimes is the inference more accurate, and vice versa? Provide intuitive/qualitative explanations for your observations. \n",
    "\n",
    "- Repeat the above for the step model, with the following modifications. The aspect of the step model's hidden states that we really care about is whether or not the ''neuron\" has jumped to the upper rate level. Calculate the probability of being in the upper rate level based on the posterior state probabilities $P(s_t | n_{1:T})$, and again make plots of it for various simulated spike-train trials. Visualise the true jump time on these plots. You can take the time point when the posterior probability of being in the upper rate level exceeds 0.5 as the estimated/inferred jump time, and mark that on the plots as well.**Â§Â§Â§**<br>\n",
    "For your report, try to combine different trials in one plot or figure, in a compact but nice way.<br> Based on these plots, comment (include both accounts of your observations and your qualitative explanations for them) on the accuracy  of the inference in different regions of the model parameter space. \n",
    "\n",
    "- The `hmm_expected_states` function has an optional boolean input `filter` which is false by default. When true, the function calculates $P(s_t | n_{1:t})$ instead of $P(s_t | n_{1:T})$: i.e. the posterior conditioned only on observations up to and including the \"current\" time-step $t$. This corresponds to the so-called filtering problem (as in the Kalman filter; the default case, `filter = False`, corresponds to \"smoothing\". Filtering is appropriate for applications where inference has to be performed online, in which case, to infer $x_t$ we do not have the luxury of having access to future observations -- without a time-machine, that is! In our case (as engineers studying the computational mechanisms in area LIP), we do of course have access to the entire spike-train; hence the default option. Nevertheless, carry out a theoretical study of the differences in inference accuracy, for both models, using smoothing vs. filtering. Again comment on your observations of the dependence of the difference between the respective errors of smoothing and filtering, in each case, in different regions of the model parameter space (again including $R_h$ and $x_0$). Try to provide qualitative explanations for these observations. \n",
    "\n",
    "In all of the above sub-tasks,  quantiatify the latent-state inference accuracy by evaluating the average error (both over trials and over time, if the latter makes sense) of the posterior estimates of the $x_t$ trajectories (for the ramp model) or the jump times (for the step model). You can then make heatmap or contour plots of these errors as a function of two parameters -- and different plots for different choices of parameter pairs. (The `tricontourf` function of `matplotlib` is very useful for this purpose.)\n",
    "\n",
    "\n",
    "**Â§:** The module `inference.py` makes use of the [Numba package](http://numba.pydata.org/) to speed up computations by so-called just-in-time (JIT) compilation.  Numba can be imported in Colab, but to use it locally, you will need to install it, following these [instructions](https://numba.readthedocs.io/en/stable/user/installing.html).\n",
    "Also note that JIT makes a function run slowly the first time you use it. So the first time you run a function (`hmm_expected_states`\n",
    "\n",
    "\n",
    "**Â§Â§:** This (log) probability depends implicitly on model parameters, hence the name (log) likelihood; it is called `normalizer` in the code for reasons having to do with the fact that it normalizes the message products involved in the FBT.\n",
    "\n",
    "**Â§Â§Â§:** (Optional -- no extra marks!) Let $y_t$ be a reduced version of $s_t$ with $y_t = 1$ or 0 indicating being or not being in the upper rate level. By exploiting the deterministic relationship beween the $y_t$ and the jump time, $\\tau$, a linear equation can be found which relates the posterior distribution of $\\tau$ with the posterior probabilities of $y_t$ at different time-steps (which you have calculated). This linear system of equations can then in principle be solved, based on which you can calculate the posterior distribution and therefore the posterior expectation of $\\tau$, and use that as an estimate instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b6abda-02d4-4fc0-8929-3883b3b38536",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"colab\" # change this to \"local\" if you are on Deepnote or your own computer\n",
    "\n",
    "if mode == \"local\":\n",
    "    import inference\n",
    "elif mode == \"colab\":\n",
    "    import requests\n",
    "    url = 'https://github.com/ahmadianlab/gg3_nda/blob/main/inference.py?raw=true'\n",
    "    r = requests.get(url)\n",
    "    with open('inference.py', 'w') as f:\n",
    "        f.write(r.text)\n",
    "    import inference\n",
    "else:\n",
    "    raise Exception(\"mode must be either local or colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eec08-1240-4a21-bd24-060e459b54a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1f063ef-edaa-4096-96af-574b8594bd08",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ebefd-e7d3-48c4-bc96-eae663f8c4ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d11ef8b8-33aa-4eae-8b27-82020c6a274f",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb9b44-b77c-475c-a039-2e04cacffdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
